services:
  dots-ocr-vllm:
    build:
      context: .
      dockerfile: Dockerfile
      # No build args needed for the HF token; pass it at runtime below
    container_name: dots-ocr-vllm
    ports:
      - "8000:8000"

    # Mount the host HF cache so you don't re-download the model each time
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface

    # If the model is private or needs a token, set HF_TOKEN in a .env file
    environment:
      HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
      # Optional overrides:
      # VLLM_MODEL: rednote-hilab/dots.ocr
      # VLLM_HOST: 0.0.0.0
      # VLLM_PORT: "8000"

    ipc: host
    restart: unless-stopped

    # Healthcheck waits for the model server to be up
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 300s

  mineru-vllm:
    build:
      context: .
      dockerfile: Dockerfile.mineru
      # No build args needed for the HF token; pass it at runtime below
    container_name: minerU-2.5
    runtime: nvidia
    ports:
      - "8001:8001"

    # Mount the host HF cache so you don't re-download the model each time
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface

    # GPU configuration for NVIDIA runtime
    environment:
      HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
      VLLM_MODEL: "opendatalab/MinerU2.5-2509-1.2B"
      VLLM_HOST: "0.0.0.0"
      VLLM_PORT: "8001"
      NVIDIA_VISIBLE_DEVICES: all

    # Enable GPU access
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    ipc: host
    restart: unless-stopped

    # Healthcheck waits for the model server to be up
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 300s
